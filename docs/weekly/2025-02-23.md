---
title: Mine Weekly 第 62 期
description:
outline: [2, 3]
hidden: true
date: 2025-02-23
recommend: -83
---

## 🚀 科技动态

> 最新的科技进展和市场动态，包括新闻、行业动态、趋势分析

### 马斯克 20 万块 GPU 炼出 Grok-3

> https://mp.weixin.qq.com/s/YDjin48P-qsxPv48mpCbtg

马斯克领导的xAI团队推出了Grok-3大模型，该模型使用20万块GPU训练，凭借强大的数学、推理能力和推理过程展示，在多项基准测试中超越了DeepSeek-R1和GPT-4o等模型，并在LMSYS Arena排行榜上以Elo评分超1400位列第一，同时推出了推理模型Grok-3 Reasoning、AI智能体DeepSearch等功能，马斯克表示将在几个月内全面开源Grok-3。

### Grok 3是否意味着大力出奇迹的大模型法则仍然成立？
> https://weibo.com/1064649941/PeT6Asavy

这篇文章探讨了大模型发展中的「大力出奇迹」Scaling Law 法则是否依然有效的问题。
通过分析预训练、RL（强化学习）和Test Time（测试时扩展）三个阶段的Scaling Law（规模扩展法则），作者指出：不同阶段的性价比差异决定了技术路线选择。Grok 3通过大幅增加预训练算力推大基座模型，看似违背性价比原则，但可能是为了提升后续RL阶段的天花板。这暗示不同阶段的扩展法则可能存在依赖关系，共同推动模型能力的持续进化。

摘要
1. **预训练阶段Scaling Law依然有效但性价比低**：数据不足时只能通过增大模型规模提升效果，但成本过高；
2. **技术路线选择依赖性价比排序**：Test Time扩展（如思维链推理）> RL扩展（如强化学习）> 预训练扩展；
3. **Grok 3的策略逻辑**：通过推大基座模型规模，可能为后续RL阶段效果提升奠定基础；
4. **模型能力的循环进化**：当Test Time和RL扩展遇瓶颈时，可回归预训练扩展提升天花板，形成持续迭代。

核心观点
- 🔄 **Scaling Law的阶段性特征**：
  - 预训练阶段扩展（数据/模型规模）仍是最后手段，但成本效益最低；
  - RL和Test Time扩展是目前性价比更高的技术路线。
- 💡 **Grok 3的潜在逻辑**：
  - 基座模型规模增大可能提升后续RL阶段效果的天花板；
  - 模型能力进化存在「预训练→RL→Test Time→预训练」的循环依赖。
- ⚖️ **实践权衡**：
  - 卡多加速实验迭代，但预训练阶段过度堆算力可能得不偿失；
  - 开源大模型部署困难（如Deepseek R1）反映基座规模与应用成本的矛盾。
- 🌐 **行业趋势**：
  - 蒸馏深度思考数据（如Deepseek V3的「左脚踩右脚」模式）成为提升基座能力的低成本方案；
  - 多模态数据对模型智商提升作用有限，文本数据规模或已接近上限。

## 💻 技术文章

> 各种技术的应用和实践方法，包括各种技术文章、教程、实践案例等内容

### Cursor 编程的 15 条经验建议

1. 从模板开始：通过从 GitHub 或其他来源克隆模板来开始项目，以提供坚实的基础。（在 Cursor 中，从 Repo 开始，并粘贴此链接以构建预置 AI 功能、数据库和授权的 nextjs 应用：https://github.com/ansh/template-2）

2. 使用智能体模式：使用 Cursor 的智能体模式（而不是普通模式）来通过自然语言命令创建、编辑和管理文件。

3. 使用 Perplexity：使用 perplexity 从网络上查找新的设计和 API。说明你正在创建一个 nextjs 项目，并且你想要创建某个功能，让它为你提供说明和代码示例。

4. 在 Composer 中创建新对话：为每个不同的任务开启新的 Composer 对话，保持智能体对话简短。

5. 本地运行，频繁测试：使用内置服务器在本地运行应用并经常测试，以尽早发现问题。

6. 迭代和改进：拥抱快速迭代—初期不要过于追求完美设计，而是逐步改进。

7. 使用语音转文本：使用像 Whispr Flow 这样的工具来实现更快的输入，保持轻松。

8. 明智地克隆和分叉：使用 GitHub 仓库作为起始模板来加速开发或寻找灵感，然后根据你的愿景进行定制。

9. 将错误复制给智能体：当出现错误时，将控制台的错误信息复制并粘贴到 Composer 智能体中，大多数情况下都能得到解决。处理错误时，如果第一次没有解决，详细解释问题。

10. 记得可以恢复之前的对话：经常保存你的工作，以便在需要时可以恢复到之前的状态。

11. 保护你的密钥：始终将 API 密钥和敏感数据存储在环境文件中，而不是硬编码。

12. 经常提交：定期将进度推送到 GitHub 以跟踪更改和保护你的工作。Cursor 可以帮你完成这个，只需询问智能体。

13. 尽早部署：使用像 Vercel 这样的平台尽早部署你的应用，以确保部署时没有错误。

14. 记录并重用有效的提示语：记录最有效的提示，以便未来的开发和调试更容易。

15. 享受过程：拥抱轻松编程的创作之旅，大胆尝试，不断学习，享受其中的乐趣。保持轻松。

### 从第一性原理出发来介绍 AI Agent

> https://goyalpramod.github.io/blogs/AI_agents_from_first_principles/

一篇AI代理的长文，文章详细介绍了AI代理的核心组件，包括提示（Prompts）、模型（Models）、工具（Tools）、记忆（Memory）以及检索增强生成（Retrieval Augmented Generation, RAG）等技术，并提供了具体的实现方法和代码示例。文章强调了在构建AI代理时应遵循的最佳实践，如最小化LLM调用、逐步迭代和优化系统性能。通过阅读本文，读者可以深入了解AI代理的构建过程，掌握如何使用Python和核心库实现复杂的AI代理系统，同时避免常见的开发陷阱。

### 低代码平台的末路-AI 编程王者归来

> https://mp.weixin.qq.com/s/NKZ6aWih3ZV8ZrMukP0BMg

作者认为，随着 AI 编程能力的飞速发展，低代码平台将逐渐被取代。尽管低代码平台通过集成 AI 功能实现了“一句话生成应用”的便捷性，但其本质仍受限于模板化和规则约束，而 AI 编程凭借灵活性、通用性及智能化优势，将成为未来开发的主流方式。

低代码平台如同"手机加速器"，在早期提升效率，但随着底层技术（AI）的成熟，其附加价值消失，甚至成为负担（如性能损耗、维护成本）。

核心逻辑对比

低代码平台

- **实现方式**  
  通过可视化界面、预设模板和脚本语言，自动化完成对象建模、数据库设计、流程配置等。
- **优势**
  - 降低开发门槛，非专业开发者可快速搭建简单应用
- **局限性**
  - 依赖模板，定制化能力不足，复杂需求下效率骤降
  - 生成代码非通用，存在平台绑定风险，可维护性差
  - 脚本语言解析消耗性能，扩展性受限

AI 编程

- **实现方式**  
  基于自然语言处理（NLP）和机器学习（ML），解析用户需求并直接生成优化代码（如 GitHub Copilot）。
- **优势**
  - 需求驱动，无需模板，适应复杂、多样化场景
  - 生成通用代码，避免平台绑定，支持全栈开发
  - 提供代码补全、错误修复、安全检测等智能化功能

关键维度对比

| 维度         | 低代码平台                         | AI 编程                                 |
| ------------ | ---------------------------------- | --------------------------------------- |
| **效率**     | 简单应用开发快，复杂需求效率低     | 自动生成代码，智能补全，整体效率更高    |
| **扩展性**   | 受限于模板，难以应对复杂需求       | 灵活生成代码，适应多样化业务场景        |
| **技能要求** | 非专业开发者易上手                 | 需一定编程基础以优化 AI 输出            |
| **成本**     | 初期成本低，但长期维护成本可能增加 | 需硬件/软件投入，长期效率提升降低总成本 |
| **大型项目** | 难以满足复杂需求                   | 生成高质量代码，支持大型项目协作        |
| **可维护性** | 平台绑定，代码维护困难             | 代码通用、可读性强，便于修改扩展        |
| **安全性**   | 依赖平台设计，存在潜在风险         | 通过安全检测生成更安全的代码            |

核心替代原因

1. **灵活性**：直接生成代码，无模板限制
2. **性能优势**：避免脚本解析开销，代码更高效
3. **开放性**：生成通用代码，避免平台锁定

未来展望

- **低代码剩余价值**：仅适用于零代码场景（非技术用户快速搭建简单应用）
- **AI 主导地位**：在复杂项目、专业开发中成为主流，推动智能化转型

结论与建议

- 建议开发团队尽早转向 AI 编程实践，尤其是大型项目
- 充分利用 AI 的灵活性、高效性和可维护性优势
- 低代码平台可能继续服务非技术用户，但在专业开发领域将逐渐被取代

### 推理类模型的最佳实践

> https://platform.openai.com/docs/guides/reasoning-best-practices

OpenAI 官方博客发布，指导大家如何更好的使用 o1、o3 这类推理模型，当然也可以应用在 deepseek r1 上。

什么时候使用推理模型？

1. 处理模糊任务
2. 处理大量非结构化信息
3. 在大型数据集中寻找关系和细微差别
4. 多步骤
   怎么有效的用推理模型？
5. 保持提示简单直接
6. 避免思维链提示
7. 使用分隔符以提高清晰度：使用分隔符（如 Markdown、XML 标签和章节标题）来清楚地指示输入的不同部分，这有助于模型正确地解释各个部分。
8. 首先尝试零样本，如果需要再尝试少样本：推理模型通常不需要少样本示例就能产生好的结果，所以首先尝试编写没有示例的提示。 如果你对期望的输出有更复杂的要求，在提示中包含一些输入和期望输出的示例可能会有所帮助。但要确保示例与你的提示指令非常一致，因为两者之间的差异可能会导致不良结果。
9. 提供具体的指导方针： 如果你想明确地限制模型的响应（例如“提出一个预算低于 500 美元的解决方案”），请在提示中明确地列出这些约束条件。
10. 非常明确地说明你的最终目标： 在你的指令中，尝试为成功的响应提供非常具体的参数，并鼓励模型持续推理和迭代，直到符合你的成功标准。

### 如何更好的为 DeepSeek R1 或 OpenAI o1 这样的推理模型写提示词？

> https://techcommunity.microsoft.com/blog/azure-ai-services-blog/prompt-engineering-for-openai%E2%80%99s-o1-and-o3-mini-reasoning-models/4374010

1. 保证提示清晰且具体
   明确说明你想让模型完成什么。避免不相关的信息。如果问题复杂，可直接简要陈述，不要同时抛出多个话题或做过多背景描述。

2. 必要的上下文要提供，不相关的要省略
   包含模型所需的领域信息或数据（如案例、事实），因为模型未必具备最新或小众知识；但别堆砌与任务无关的材料或一堆示例，以免干扰。

3. 尽量零示例或极少示例
   优先采用零示例模式。只有当模型理解有误或者格式不对时，才加入简短的示例作为演示。O1/O3 本身不需要像旧版 GPT 那样大量示例来引导。

4. 使用 System/Developer 指令定位角色与风格
   比如「你是一位法律分析师」，或「请做一名数学老师给学生讲解」，从而设置合适的专业度和语气；再如「请用条列式列出答案」，指定输出结构。

5. 通过指令控制回答长度与详细程度
   若要简短回答，就写「限一段话内给出结论」；若要详细分析，就写「请详述你的推理过程」。O1 默认会倾向详尽，但你可以覆盖该默认。

6. 在 O3-mini 上使用“推理努力程度”参数
   （若 API 允许）根据任务需求设置低/中/高，以在速度与准确性之间做平衡。

7. 避免重复的“逐步思考”指示
   不必告诉 O1/O3「让我们一步步思考」，因为它们已在内部做链式推理；这类指令对 GPT-4o 更有效。只有当你想要输出“所有中间步骤”时才额外声明。

8. 测试和迭代
   如果初始回答不理想，可以改变提示表述或更精确地说明需求。虽然 O1/O3 通常一次就能给出高质量解答，但微调提示仍能进一步提升可读性或输出形式。

9. 对重要结论做验证
   对于需要高可靠度的回答，可进行追问或多次查询，并对比不同答案或让模型自检，以增强对结果的信心。即便是 O1 也有可能出错，务必审慎使用。

### 如何构建靠谱的 AI agent？

> https://www.anthropic.com/research/building-effective-agents

这是一篇来自于 Anthropic Claude 团队的座谈交流。他们围绕 agents 的定义、实践经验与未来展望展开了一个深入对话。

文章从理论到实践全面阐述了 AI agent 的开发要点：

1. 首先明确区分了 agent 和工作流的本质区别，agent 具有自主决策能力而非简单的固定流程；
2. 在代码实现层面，详细对比了工作流的线性特征和 agent 的灵活性；同时强调了开发者需要深入理解模型视角，在设计时保持同理心。
3. 在工具设计方面，文章指出了开发者常见的忽视工具接口友好性的误区。
   对于 agent 的应用前景，文章认为目前消费级 agent 被过度炒作，存在偏好表达复杂和风险控制等挑战，而企业级应用则更具潜力，特别适合自动化重复性任务。
   最后，专家们建议开发者建立完善的度量体系，并着眼于开发能随模型进步而持续改进的产品。

我印象比较深刻的有三句话：

1. Agents 不是工作流，而是管理工作流的自动化流程；
2. 对 Agents 工作要有同理心，人要站在机器的视角考虑流程是否奏效；
3. 最后一句，是奥特曼也说过很多次的，构建的工具要考虑模型演进，如果你的工具在模型演进之后失去意义了，那么即是坏选择，如果模型越好，你的工具越好，就是好选择。

## 🔧 工具资源

> 如何使用这些工具提高工作效率，包括各种技术工具、软件、插件等方面的内容，各种技术资源、免费课程、学习资料

### AI系统
> https://chenzomi12.github.io/index.html


###  maxiee 的 AI 全栈项目
> https://github.com/maxiee/RaySystem

AI 全栈项目，是一个用于打造超级个体的工作流，涵盖 RaySystem Server（Python）、RaySystem Client（Flutter）、以及精挑细选的优秀软件，组成一个完整生态。

本开源项目主要是跟大家一起探讨和学习人工智能、深度学习的系统设计，而整个系统是围绕着在 NVIDIA、ASCEND 等芯片厂商构建算力层面，所用到的、积累、梳理得到 AI 系统全栈的内容。希望跟所有关注 AI 开源项目的好朋友一起探讨研究，共同促进学习讨论。

课程主要包括以下六大模块：

第一部分，AI 基础知识和 AI 系统的全栈概述的AI 系统概述，以及深度学习系统的系统性设计和方法论，主要是整体了解 AI 训练和推理全栈的体系结构内容。

第二部分，硬核篇介绍AI 芯片概况，这里就很硬核了，从芯片基础到 AI 芯片的范围都会涉及，芯片设计需要考虑上面 AI 框架的前端、后端编译，而不是停留在天天喊着吊打英伟达，被现实打趴。

第三部分，进阶篇介绍AI 编译器原理，将站在系统设计的角度，思考在设计现代机器学习系统中需要考虑的编译器问题，特别是中间表达乃至后端优化。

第四部分，实际应用推理系统与引擎，讲了太多原理身体太虚容易消化不良，还是得回归到业务本质，让行业、企业能够真正应用起来，而推理系统涉及一些核心算法和注意的事情也分享下。

第五部分，介绍AI 框架核心技术，首先介绍任何一个 AI 框架都离不开的自动微分，通过自动微分功能后就会产生表示神经网络的图和算子，然后介绍 AI 框架前端的优化，还有最近很火的大模型分布式训练在 AI 框架中的关键技术。

第六部分（这部分还没写），汇总篇介绍大模型与 AI 系统，大模型是基于 AI 集群的全栈软硬件性能优化，通过最小的每一块 AI 芯片组成的 AI 集群，编译器使能到上层的 AI 框架，训练过程需要分布式并行、集群通信等算法支持，而且在大模型领域最近持续演进如智能体等新技术。

### aman.ai 专注于AI知识分享
> https://aman.ai/

这个不是那种随便从网站拷贝一些文章的网站。每一篇都是维护者Aman Chadha（AWS的工程师）写的长文，很多图表和对比数据，尝试为你讲清楚这个概念。比如介绍deepseek r1的文章，直接整了1万多个单词，从MoE、MLA、MTP到强化学习，再到deepseek各个不同版本的对比之类都有。

### AI Agents for Beginners 微软出了一门给初学者学习的 AI 智能体课程
> https://github.com/microsoft/ai-agents-for-beginners

共 10 节课程，涵盖构建 AI 智能体的所有基础知识，旨在教授我们从零开始构建一个 AI 智能体。课程内容已做了中文翻译，学习起来更加轻松，同时提供每节课所使用的示例代码，方便我们运行。

### DeeSearcher 开源版 Deep Research

> https://github.com/zilliztech/deep-searcher

结合了大模型和向量数据库，为个人/企业知识管理、智能问答、信息搜索等场景提供高度准确的答案和全面分析。

支持本地文件导入，以及网络数据爬取等文档加载，支持 DeepSeek、OpenAI 等大模型。

### LangBot

> http://github.com/RockChinQ/LangBot

可扩展的通信机器人平台，提供了丰富生态、支持扩展、多模态的能力，用于构建大模型的即时通信机器人，支持接入 QQ、微信、飞书等平台。

同时支持多种主流大模型，如 DeepSeek、ChatGPT、Claude、Gemini、Ollama 等。

除此之外，还支持访问控制、限速、敏感词过滤等功能，支持多种部署方式，而且配置简单。

### Llama Tutor

> https://github.com/Nutlope/llamatutor

一款开源免费的 AI 个人导师工具。

集成 Llama 3.1 和 Serper 为工具提供强大的推理以及搜索能力，以达到高质量辅助教学。

输入任何想要了解的知识，以及选择能接受的教育水平，覆盖了小学到大学，即可为你生成个性化导师，解答一切疑难杂题。

### Company Researcher

> https://github.com/exa-labs/company-researcher

快速了解任何(外国)公司的开源 AI 工具：Company Researcher。

只需输入公司的网站链接，即可帮我们从互联网上全面收集关于该公司的信息，包括组织架构、产品、融资情况以及官方媒体账号等等信息。

抓取的数据主要来自各大平台开放的 API，如 Linkedin、X（Twitter）、Yogithub.com/exa-labs/company-researcheruTube、GitHub、Reddit 等。

最终汇总罗列一个详细面板给我们快速了解该家公司，不过从数据来看，主要面向国外公司。

### AudioNotes

> https://github.com/harry0703/AudioNotes

快速将音视频转结构化笔记的开源工具：。

基于 FunASR 和 Qwen2 构建，可快速提取音视频内容，并利用大模型能力整理成一份结构化的 Markdown 笔记，方便快速阅读。

工具支持搭配 Ollama 使用本地模型，并提供了 Docker 快速部署方式。

### Scira 开源 AI 搜索引擎

> https://github.com/zaidmukaddam/scira

集成 DeepSeek、Claude、Grok 等模型以及各种 API，实现了实时天气查询、代码片段运行、地图导航、学术搜索、视频搜索、X 帖子搜索等等功能。同时，提供简洁好用的可视化界面，并能为我们清晰展示相关搜索到的内容。可以通过 Vercel 快速部署使用。
