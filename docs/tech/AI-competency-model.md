# AI 人才模型

## 基础 AI 应用能力

### 提示词工程能力

懂得用最好的方式问问题，引导AI高效输出

- [提示词技巧手册](https://juejin.cn/post/7274626136328142900)
- [常用提示词模板](https://juejin.cn/post/7324250434910011442)
- [AI 提示词工程总结：SFT vs RL，从"教步骤"到"划重点"](https://juejin.cn/post/7470102625934983218)
- [推理模型提示词编写指南](https://juejin.cn/post/7472575677545381922)
- [如何用好推理型AI？这份提示词设计指南请收好](https://juejin.cn/post/7473383458992259107)


### 熟练使用 AI 工具

音频转文本、 Trae、Cursor

> Gemini Flash 2.0 用于 OCR  
> Sonnet 3.7 用于编程  
> GPT-4o 用于写作  
> o3-high 用于推理  
> Flux Pro Ultra 用于图像生成  
> 可灵 1.6 用于视频生成  
> Eleven Labs 用于音频处理  
> Gemini Flash 用于图像编辑  
> GPT-4o 用于语音模式  
> Deepseek R1 和通义千问，因为我们热爱开源

### 内容评估能力

知道 AI 生成的内容是否靠谱，能有效验证和优化

## 高级 AI 应用能力

### 任务拆解能力
知道哪些工作适合用AI，哪些环节应该自己把控

### 业务场景应用能力

知道如何用AI真正提高工作效率，重新设计现有业务流程，引入 AI 的智能，让原来需要很多手动干预的操作，变成半自动或者全自动的流程，极大提升效率。不但会用AI工具，更要知道「什么时候用」「怎么用」「怎么用得更好」。

## 不同领域考察 AI 能力

### AI 辅助编程

程序员面试，以前都是让候选人徒手写代码，现在AI时代来了，光凭手写未必真实。

更好的办法是：直接让候选人现场用 AI 工具（比如 Trae、Cursor 或 Windsurf 等）解决一个小问题。

关键不在于 AI 写了多少代码，而是：

- 他怎么问 AI？
- 如何判断 AI 写的代码对不对？
- 遇到问题怎么调整？
考察的是程序员利用 AI 的真实技能，而不是死记硬背的能力。

### AI 生成创意和内容
营销岗位尤其看重创意，但很多创意初稿现在都能让 AI 快速生成了。

面试时，不妨这么做：

- 给候选人一个产品，让 TA 用 AI 生成一条营销文案。
- 然后问 TA：「你为什么用这个 Prompt？」、「AI 给的文案你怎么看？」

真正的考察点在于：

- 候选人会不会灵活地调整提示词；
- 是否能判断 AI 内容质量，主动修正不足之处；
- 对 AI 生成内容是否有自己的标准（比如品牌调性、目标用户偏好）。

### 用 AI 做用户洞察和数据分析

产品经理的日常工作经常和数据打交道。

比如，你可以现场出一道题：「给你一份用户反馈数据，现场用 AI 工具帮我们提炼出产品改进建议。」


考察候选人：

- 是否懂得怎么使用 AI 快速抓住用户的痛点；
- 有没有能力验证 AI 结论的准确性；
- 是否能结合 AI 建议，提出清晰明确的改进方案。

## AI 的局限性
优秀的人才会主动提到 AI 的局限性、使用的风险、以及如何有效避免这些风险。而不够熟悉的人通常只是机械地使用工具，甚至完全忽略了 AI 可能产生的误导性结果。

这里给大家分享一些实操性强的通用AI面试题：

- 「你遇到过 AI 给出明显错误答案的情况吗？你怎么处理的？」
- 「最近有没有新出的 AI 工具或功能是你学习并实际应用了的？具体讲讲。」
- 「你平时怎么调整 Prompt 来优化 AI 输出？」
- 「如果 AI 生成的内容和你的预期不符，你会怎么优化它？」
- 「你觉得 AI 目前有哪些无法解决的业务难题？遇到这些难题你会怎么办？」
这些问题看似简单，但候选人的回答可以充分展示：

- TA 有没有真正用过 AI；
- TA 是否能批判性地看待 AI 工具；
- TA 是否具备快速学习和自我优化的意识。




• 问题一：从何入手？ 你知道 AI 很强大，但具体到你的业务场景，应该从哪里开始？是做一个聊天机器人，还是做数据分析工具？抑或是更高级的 AI Agent？
• 问题二：如何评估效果？ 你花了大量时间和资源搭建了一个 AI 系统，但如何判断它是否真的有效？是看用户反馈，还是看技术指标？评估的标准是什么？
• 问题三：技术选择困惑。 你听说过微调（fine-tuning），觉得这听起来很高大上，是不是应该一上来就用微调来提升模型性能？



常见错误：过早考虑微调，忽视基础优化

在 AI 应用中，微调（fine-tuning）是一个常被提及的技术。许多企业一听到微调，就觉得这是提升模型性能的“灵丹妙药”，急于尝试。然而，Anthropic 警告，微调并不是万能的，而且往往不是最佳选择。

微调的误区

1. 微调不是“银弹”。 微调相当于对模型进行“脑外科手术”，会影响模型在其他领域的推理能力。盲目微调可能导致模型在某些任务上表现更好，但在其他任务上表现下降。
2. 微调成本高昂。 微调需要大量的数据和计算资源，而且成功率参差不齐。很多时候，企业投入了大量资源，却未能获得预期的效果。
3. 忽视基础优化。 在没有充分评估和优化基础模型的情况下，过早考虑微调，往往是本末倒置。

何时考虑微调？

建议，只有在基础优化无法满足需求时，才考虑微调。具体来说：

• 先尝试提示工程（prompt engineering）。 通过优化提示词，提升模型在特定任务上的表现。
• 利用其他技术。 例如，提示缓存（prompt caching）可以降低成本和延迟，检索增强（retrieval augmentation）可以提高模型的知识覆盖面。
• 评估后再决定。 在充分评估后，如果发现基础模型无法满足需求，再考虑微调。

其他实用建议

除了上述最佳实践，Anthropic 还分享了一些其他实用建议，帮助企业更好地落地 AI 应用。

• 构建代表性的评估集。 确保你的评估集覆盖了各种可能的情况，包括边缘案例。例如，在客服场景中，不仅要评估常见问题，还要评估无关或恶意的问题。
• 监控和回放。 建立完善的监控系统，记录 AI 系统的表现，并定期回放评估结果，持续优化。
• 探索创新架构。 例如，AI Agent、上下文检索等，可以提升 AI 应用的智能度和用户体验。


权衡“智能度、成本、延迟”，找到最优平衡

在 AI 应用中，企业往往需要在“智能度、成本、延迟”这三个维度之间进行权衡。而 Anthropic 的建议是：很少有企业能够同时在这三个方面都做到极致，因此，明确你的核心需求，找到最适合的平衡点至关重要。

• 智能度： AI 模型的准确性和智能水平。
• 成本： 开发和运维 AI 系统的经济成本。
• 延迟： AI 系统响应的速度。

不同的应用场景对这三个维度的要求不同。例如：

• 客服场景： 延迟是关键指标。用户希望在 10 秒内得到回复，否则可能会流失。因此，在客服应用中，快速响应比极高的智能度更重要。
• 金融研究员助手： 智能度是核心。金融决策需要高度准确的信息，响应时间可以适当放宽。

如何找到平衡？

1. 明确核心指标。 根据业务需求，确定哪个维度是最关键的。
2. 设计评估标准。 针对核心指标，设定明确的评估标准。
3. 灵活调整。 在开发过程中，根据评估结果，灵活调整技术方案，找到最优平衡。

例如，在客服场景中，你可以通过设计“思考中”的动画或引导用户阅读其他内容来缓解延迟问题，同时优化模型以提高响应速度。
